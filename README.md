# ğŸ¤– Introduction to Machine Learning â€“ Applied Labs & Projects

### ğŸ‘¨â€ğŸ’» **By:** [**Parv Patel**](mailto:parvpatel.data@gmail.com)

ğŸ“ **B.Tech in Computer Science (Indian Institute of Technology, Palakkad)**
ğŸ“§ **[parv4careers@gmail.com](mailto:parv4careers@gmail.com)** | ğŸ’¼ [**LinkedIn**](https://linkedin.com/in/parvptl) | ğŸ§  [**GitHub**](https://github.com/Parvptl)

---

## ğŸ¯ **Objective**

This repository contains a comprehensive collection of **Machine Learning laboratory experiments and mini-projects** developed during the course *Introduction to Machine Learning*.
It demonstrates both **theoretical understanding and practical implementation** of machine learning algorithms â€” from foundational regression models to advanced probabilistic systems like HMMs.

> ğŸ§© Focus: Bridging mathematical intuition with hands-on implementation.

---

## ğŸ“˜ **Course Overview**

### ğŸ§  **Topics Covered**

* **Mathematical Foundations:**
  Linear algebra review (vector derivatives), probability theory, and Bayesâ€™ theorem.
* **Supervised Learning:**
  Linear & Non-linear regression, Logistic Regression, Naive Bayes, SVMs.
* **Unsupervised Learning:**
  Clustering (K-Means, GMM, DBSCAN).
* **Ensemble Methods:**
  Decision Trees, Random Forests, Boosting, and Bagging.
* **Dimensionality Reduction:**
  PCA and Canonical Correlation Analysis.
* **Sequential Models:**
  Hidden Markov Models (HMMs) and temporal sequence learning.
* **Model Evaluation & Selection:**
  Cross-validation, LOO error, ROC Curves, and statistical significance testing.

---

## ğŸ§© **Lab Experiments & Progress**

| **Date**   | **Topic**                         | **Highlights**                                                                  |
| ---------- | --------------------------------- | ------------------------------------------------------------------------------- |
| **1 Aug**  | Python & NumPy                    | Array operations, vectorization, broadcasting, and computation efficiency       |
| **8 Aug**  | SciPy & Pandas                    | Data manipulation, preprocessing, and preview of a previous yearâ€™s ML project   |
| **22 Aug** | Linear Regression                 | Implemented from scratch; analyzed impact of feature standardization            |
| **29 Aug** | Ridge & Non-linear Regression     | Cross-validation (K-Fold, LOO); explored bias-variance tradeoff                 |
| **2 Sep**  | Logistic Regression & Naive Bayes | Built classifiers from scratch and compared decision boundaries                 |
| **12 Sep** | Kernel SVM                        | Implemented kernelized SVM for non-linear separation using Gaussian kernels     |
| **17 Oct** | Decision Trees & Random Forests   | Built ensemble models and visualized feature splits and Gini importance         |
| **24 Oct** | PCA                               | Reduced dimensionality of high-dimensional data and analyzed variance retention |
| **31 Oct** | Clustering (K-Means, GMM, DBSCAN) | Applied unsupervised learning; compared density vs centroid-based clustering    |
| **7 Nov**  | Hidden Markov Models (HMM)        | Implemented sequence learning using probabilistic state transitions             |

---

## ğŸ“Š **Core Concepts Strengthened**

### ğŸ“ˆ Regression Analysis

* Linear and Ridge Regression
* Non-linear curve fitting and regularization
* Error metrics: MSE, RMSE, and cross-validation-based model validation

### ğŸ” Classification Techniques

* Logistic Regression (Sigmoid, Cross-Entropy Loss)
* Naive Bayes (Probabilistic text and categorical classification)
* SVM (Linear, Polynomial, RBF Kernel-based classification)

### ğŸŒ³ Ensemble Learning

* Decision Trees (Entropy, Gini Impurity)
* Random Forests and Bagging Techniques
* Boosting concepts for variance reduction and performance improvement

### ğŸ”¢ Dimensionality Reduction

* Principal Component Analysis (PCA)
* Canonical Correlation Analysis (CCA)
* Visualization of reduced feature space and explained variance ratios

### ğŸŒ€ Clustering Algorithms

* K-Means (Euclidean-based partitioning)
* Gaussian Mixture Models (Probabilistic EM clustering)
* DBSCAN (Density-based noise-resistant clustering)

### ğŸ§® Sequential Models

* Hidden Markov Models (Forward-Backward, Viterbi Algorithm)
* Applications in speech recognition and sequential event prediction

### âš–ï¸ Model Evaluation & Selection

* K-Fold and Leave-One-Out Cross Validation
* ROC Curves, Precision-Recall tradeoff
* Statistical significance and error analysis

---

## ğŸ§° **Technical Stack**

| Category                                  | Tools / Libraries                                   |
| ----------------------------------------- | --------------------------------------------------- |
| **Languages**                             | Python                                              |
| **Data Handling**                         | NumPy, Pandas, SciPy                                |
| **Modeling & ML**                         | scikit-learn, statsmodels                           |
| **Visualization**                         | Matplotlib, Seaborn                                 |
| **ML Concepts Implemented from Scratch**  | Linear/Logistic Regression, Naive Bayes, Kernel SVM |
| **Clustering & Dimensionality Reduction** | PCA, K-Means, GMM, DBSCAN                           |
| **Sequential Learning**                   | Hidden Markov Models (HMMs)                         |
| **Evaluation Metrics**                    | Cross-validation, ROC, Precision, Recall, F1-score  |

---

## ğŸ’¡ **Learning Outcomes**

âœ… Strong understanding of supervised and unsupervised learning fundamentals.
âœ… Ability to implement ML algorithms **from scratch** and validate with existing libraries.
âœ… Proficiency in **model evaluation**, cross-validation, and tuning hyperparameters.
âœ… Familiarity with probabilistic models and sequential data handling.
âœ… Exposure to **real-world ML applications** and end-to-end experimentation.

---

## ğŸ”¬ **Projects & Applications**

* ğŸ§® **Regression Pipeline:** Built linear and ridge regression models with standardization effect analysis.
* ğŸ¤– **Binary Classifier:** Logistic regression from scratch using gradient descent optimization.
* ğŸŒ **SVM Kernel Mapping:** Implemented Gaussian kernel transformation for non-linear boundaries.
* ğŸŒ² **Random Forest Ensemble:** Visualized feature splits and feature importances.
* ğŸ“‰ **Dimensionality Reduction:** PCA-based visualization for 2D projections.
* ğŸ§  **HMM Sequence Prediction:** Implemented probabilistic model for temporal data.

---

## ğŸ—ï¸ **Course Integration**

This course bridges **mathematical intuition** and **computational machine learning** by emphasizing:

* Core ML algorithm derivations
* Statistical model validation
* End-to-end model pipeline creation
* Practical implementation using real-world datasets

> ğŸš€ The goal: **From theory to practical ML proficiency.**

---

## ğŸŒŸ **Key Strengths**

âœ… Algorithmic foundation with mathematical rigor
âœ… Deep understanding of model bias, variance, and generalization
âœ… Experience with high-dimensional feature reduction and clustering
âœ… Practical exposure to both deterministic and probabilistic ML systems

---

## ğŸ“¬ **Contact**

ğŸ“§ **Email:** : [parv4careers@gmail.com](mailto:parvpatel.data@gmail.com)
ğŸ’¼ **LinkedIn:** : [linkedin.com/in/parvptl](https://linkedin.com/in/parvpatel-data)
ğŸ“‚ **GitHub:** : [github.com/Parvptl](https://github.com/Parvptl)


---

### ğŸ§© *â€œBuilding intelligent systems through mathematical precision and computational learning.â€*

#### ğŸ *Built with Python, Data Curiosity, and Machine Learning Fundamentals*
